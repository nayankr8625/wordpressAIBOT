{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nayankumarjha/Desktop/WordPressBot/wordpressAIBOT/.venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/Users/nayankumarjha/Desktop/WordPressBot/wordpressAIBOT/.venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from data_collection import extract_content_as_json, fetch_latest_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fetch_latest_posts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m orignal_post \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_latest_posts\u001b[49m(url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://public-api.wordpress.com/rest/v1.1/sites/portfo336.wordpress.com/posts\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fetch_latest_posts' is not defined"
     ]
    }
   ],
   "source": [
    "orignal_post = fetch_latest_posts(url=\"https://public-api.wordpress.com/rest/v1.1/sites/portfo336.wordpress.com/posts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nayan Jha\n",
      "Nayan Jha\n"
     ]
    }
   ],
   "source": [
    "for post in orignal_post[\"posts\"]:\n",
    "    print(post[\"author\"][\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'post'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43morignal_post\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthor\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'post'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "post = extract_content_as_json(wordpress_domain=\"portfo336.wordpress.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Langchain: The Future of Conversational AI in WordPress Blogs'), Document(page_content='Conversational AI has made tremendous strides in recent years, with advancements in natural language processing (NLP) allowing for more human-like interactions with machines. Langchain is one of the prominent frameworks that’s at the forefront of this revolution, enabling developers and content creators to build sophisticated conversational agents. In this essay, we will explore the key features of Langchain, its applications in WordPress blogs, and the benefits it offers to bloggers and'), Document(page_content='to bloggers and website owners.'), Document(page_content='What is Langchain?\\nLangchain is a comprehensive framework designed to simplify the development of applications involving language models, such as OpenAI’s GPT-3.5 and similar AI-based models. The framework provides a set of tools and abstractions that facilitate the creation of conversational agents, automated content generation, and much more. It is designed to work with various language models, allowing developers to build scalable and robust AI applications.\\nKey Features of Langchain'), Document(page_content='Langchain is distinguished by several key features that make it an attractive choice for conversational AI development:'), Document(page_content='Modularity: Langchain is designed with modularity in mind, allowing developers to choose and configure different components according to their needs. This flexibility is crucial for creating tailored solutions.\\nInteroperability: The framework is built to work with a variety of language models and other AI tools, making it adaptable to different use cases and environments.'), Document(page_content='Extensibility: Langchain allows for easy extension and customization, enabling developers to build complex conversational agents with unique capabilities.\\nEase of Use: Despite its power and versatility, Langchain is user-friendly, with clear documentation and a supportive community, making it accessible to developers of varying skill levels.'), Document(page_content='Applications of Langchain in WordPress Blogs\\nWordPress is a leading platform for website creation and blogging, used by millions of websites worldwide. Integrating Langchain into WordPress blogs can bring a host of benefits, enhancing user engagement and providing new opportunities for content creation.\\nConversational Bots for Enhanced User Interaction'), Document(page_content='Langchain can be used to create conversational bots that interact with visitors on WordPress blogs. These bots can answer questions, guide users through the website, and even assist with customer support. By offering a more interactive experience, bloggers can keep visitors engaged and improve overall user satisfaction.\\nAutomated Content Generation'), Document(page_content='Content generation can be a time-consuming task for bloggers. Langchain can be leveraged to generate content automatically, either as full articles or as drafts that can be refined by human editors. This feature can significantly speed up the content creation process and allow bloggers to focus on other aspects of their work.\\nPersonalized Recommendations and Dynamic Content'), Document(page_content='With Langchain, it’s possible to create bots that offer personalized recommendations to users based on their interests and browsing behavior. This can lead to a more tailored user experience, with dynamically generated content that resonates with individual visitors. Such personalization can boost user engagement and encourage repeat visits.\\nImproved SEO and Content Discovery'), Document(page_content='By incorporating conversational AI into WordPress blogs, bloggers can improve their search engine optimization (SEO). Langchain can help generate content that addresses common questions and topics, potentially attracting more organic traffic. Additionally, conversational bots can guide users to relevant content, enhancing content discovery within the blog.\\nBenefits of Using Langchain for WordPress Blogs'), Document(page_content='Integrating Langchain into WordPress blogs offers a range of benefits, from improved user interaction to streamlined content creation. Let’s explore some of the key advantages.\\nEnhanced User Engagement\\nBy offering interactive conversational bots, WordPress blogs can engage visitors in new and meaningful ways. This can lead to longer visit durations, increased page views, and a more enjoyable user experience.\\nIncreased Content Productivity'), Document(page_content='Langchain’s automated content generation capabilities can help bloggers produce more content in less time. This increased productivity can be particularly valuable for bloggers who need to maintain a regular posting schedule or cover a wide range of topics.\\nBetter Customer Support'), Document(page_content='Conversational bots powered by Langchain can handle customer inquiries and support requests, providing immediate responses to common questions. This can reduce the burden on human support teams and improve customer satisfaction.\\nScalability and Flexibility\\nLangchain’s modular design allows developers to build scalable solutions that can grow with the needs of the blog. As the blog’s audience and content requirements expand, Langchain can be configured to meet those demands.'), Document(page_content='Cost-Effective Solution\\nBy automating certain tasks and reducing the need for manual content creation, Langchain can offer a cost-effective solution for WordPress bloggers. This can be especially valuable for small businesses and individual bloggers with limited resources.\\nConclusion'), Document(page_content='Langchain represents a powerful framework for building conversational AI applications, offering a wide range of features and benefits for WordPress blogs. By integrating Langchain into their websites, bloggers can enhance user engagement, streamline content creation, and improve overall user satisfaction. As conversational AI continues to evolve, Langchain is poised to play a pivotal role in shaping the future of WordPress blogs and other online platforms. Whether you’re a seasoned developer or'), Document(page_content='developer or a content creator, exploring the possibilities of Langchain can open up new opportunities for growth and innovation.')]\n",
      "Created new index for 'Langchain: The Future of Conversational AI in WordPress Blogs'\n",
      "[Document(page_content='Retrieval-Augmented Generation (RAG)\\xa0is a new approach that leverages Large Language Models (LLMs) to automate knowledge search, synthesis, extraction, and planning from unstructured data sources. This method has gained prominence over the past year due to its ability to enhance LLM applications with contextual information. The RAG data stack consists of several key components:'), Document(page_content='Loading Data: Initially, data is ingested from various sources, such as text documents, websites, or databases. This data can be in a raw or preprocessed format.\\nProcessing Data: The data undergoes preprocessing steps to clean and structure it for further analysis. This may include tasks like tokenization, stemming, and removing stop words.'), Document(page_content='Embedding Data: Each piece of data is converted into a numerical representation called an embedding. This embedding captures semantic information about the data, making it easier for the LLM to understand and process.\\nVector Database:\\xa0The embeddings are stored in a vector database, which allows for efficient retrieval based on similarity metrics. This database enables quick access to relevant data points during the generation process.'), Document(page_content='Retrieval and Prompting:\\xa0During the generation process, the LLM can retrieve relevant data points from the vector database based on the context of the current input. This retrieval mechanism helps the LLM provide more accurate and contextually relevant outputs.'), Document(page_content='Overall, the RAG approach enhances the capabilities of LLMs by enabling them to leverage external knowledge sources in a systematic and efficient manner. This can lead to more powerful and contextually aware applications in various domains, such as natural language understanding, information retrieval, and decision-making.\\nBuilding a Production grade RAG remains a complex and subtle problem. Some of the challenges associated is as follows:-'), Document(page_content='Results aren’t accurate enough:\\xa0The application was not able to produce satisfactory results for a long-tail of input tasks/queries.\\nThe number of parameters to tune is overwhelming:\\xa0It’s not clear which parameters across the data parsing, ingestion, retrieval.\\nPDFs are specifically a problem:\\xa0I have complex docs with lots of messy formatting. How do I represent this in the right way so the LLM can understand it?'), Document(page_content='Data syncing is a challenge:\\xa0Production data often updates regularly, and continuously syncing new data brings a new set of challenges.'), Document(page_content='Step 1: Planning'), Document(page_content='You might have come across various techniques aimed at improving the performance of large language models, such as\\xa0offering tips\\xa0or even jokingly threatening them. One popular technique is called “chain of thought,” where the\\xa0model is asked to think step by step, enabling self-correction. This approach has evolved into more advanced versions like the “chain of thought with self-consistency” and the generalized “tree of thoughts,”\\xa0where multiple thoughts are created, re-evaluated, and'), Document(page_content='re-evaluated, and consolidated to provide an output.'), Document(page_content='In this tutorial, I am using heavily\\xa0Langsmith, a platform for productionizing LLM applications. For example, while building the tree of thoughts prompts, I save my sub-prompts in the\\xa0prompts repository\\xa0and load them\\nStep 2: Memory'), Document(page_content='Sensory Memory:\\xa0This component of memory captures immediate sensory inputs, like what we see, hear or feel. In the context of prompt engineering and AI models, a prompt serves as a transient input, similar to a momentary touch or sensation. It’s the initial stimulus that triggers the model’s processing.'), Document(page_content='Short-Term Memory:\\xa0Short-term memory holds information temporarily, typically related to the ongoing task or conversation. In prompt engineering, this equates to retaining the recent chat history. This memory enables the agent to maintain context and coherence throughout the interaction, ensuring that responses align with the current dialogue.\\xa0In code, you typically add it as conversation history:'), Document(page_content='Long-Term Memory:\\xa0Long-term memory stores both factual knowledge and procedural instructions. In AI models, this is represented by the data used for training and fine-tuning. Additionally, long-term memory supports the operation of RAG frameworks, allowing agents to access and integrate learned information into their responses. It’s like the comprehensive knowledge repository that agents draw upon to generate informed and relevant outputs.\\xa0In code, you typically add it as a vectorized database:'), Document(page_content='Step 3: Tools'), Document(page_content='Built-in Langchain tools: Langchain has a\\xa0pleiad of built-in tools\\xa0ranging from internet search and Arxiv toolkit to Zapier and Yahoo Finance.'), Document(page_content='Custom tools: it’s also very easy to define your own tools. Let’s dissect the simple example of a tool that calculates the length of the string. You need to use the\\xa0@tooldecorator to make Langchain know about it. Then, don’t forget about the type of input and the output. But the most important part will be the function comment between\\xa0\"\"\" \"\"\"\\xa0— this is how your agent will know what this tool does and will compare this description to descriptions of the other tools:\\nStep 4: All together'), Document(page_content='I am providing a clean version of combining all the pieces of architecture together\\xa0in this script. Notice, how we can easily decompose and define separately:'), Document(page_content='All kinds of\\xa0tools\\xa0(search, custom tools, etc)\\nAll kinds of\\xa0memories\\xa0(sensory\\xa0as a prompt,\\xa0short-term\\xa0as runnable message history, and as a sketchpad within the prompt, and\\xa0long-term\\xa0as a retrieval from the vector database)\\nAny kind of\\xa0planning strategy\\xa0(as a\\xa0part of a prompt\\xa0pulled from the LLMOps system)')]\n",
      "Created new index for 'RAG using Langchain and Groq with COT'\n"
     ]
    }
   ],
   "source": [
    "create_faiss_indexes(post_data=post)\n",
    "vector_store = merge_vector_stores(vector_store_root_dir=\"doc_embedding_store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONDENSE_QUESTION_PROMPT = \"\"\"\n",
    "I want you to act as wordpress post analyzer and based on the provide context which will be the piece of my wordpress post you need to answer the question.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "from langchain.prompts import PromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "from langchain.retrievers.merger_retriever import MergerRetriever\n",
    "from typing import Tuple\n",
    "\n",
    "GROQ_API_KEY = \"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\"\n",
    "\n",
    "def query_chain(vector_store: MergerRetriever)-> Tuple[ConversationalRetrievalChain, ConversationBufferMemory]:\n",
    "    chat = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\", api_key=GROQ_API_KEY)\n",
    "    memory = ConversationBufferMemory(\n",
    "    memory_key='chat_history', return_messages=True, output_key='answer')\n",
    "    conversation_chain = (ConversationalRetrievalChain.from_llm\n",
    "                        (llm=chat,\n",
    "                        retriever=vector_store,\n",
    "                        memory=memory,\n",
    "                        return_source_documents=True,verbose=True))\n",
    "    print(\"Conversational Chain created for the LLM using the vector store\")\n",
    "    return conversation_chain,memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversational Chain created for the LLM using the vector store\n"
     ]
    }
   ],
   "source": [
    "conversation_chain,memory = query_chain(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "def runnable_chain(vector_store):\n",
    "    llm = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\", api_key=GROQ_API_KEY)\n",
    "\n",
    "    contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "    which might reference context in the chat history, formulate a standalone question \\\n",
    "    which can be understood without the chat history. Do NOT answer the question, \\\n",
    "    just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", contextualize_q_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        llm, vector_store, contextualize_q_prompt\n",
    "    )\n",
    "    qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "    Use the following pieces of retrieved context to answer the question. \\\n",
    "    If you don't know the answer, just say that you don't know. \\\n",
    "    Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "    {context}\"\"\"\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", qa_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = runnable_chain(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This post is about Retrieval-Augmented Generation (RAG), a new approach that leverages Large Language Models (LLMs) to automate knowledge search, synthesis, extraction, and planning from unstructured data sources.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "question = \"can you tell me about my post on RAG?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history, })\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg_1[\"answer\"]])\n",
    "\n",
    "second_question = \"What is this post about\"\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg_2[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'can you tell me about my post on RAG?',\n",
       " 'chat_history': [HumanMessage(content='can you tell me about my post on RAG?'),\n",
       "  'I can help you with that! Your post discusses Retrieval-Augmented Generation (RAG), a new approach that leverages Large Language Models (LLMs) to automate knowledge search, synthesis, extraction, and planning from unstructured data sources.'],\n",
       " 'context': [Document(page_content='End of Post', metadata={'title': 'Langchain: The Future of Conversational AI in WordPress Blogs', 'short_url': 'https://wp.me/pfK043-s', 'date': Timestamp('2024-05-03 14:28:41+0530', tz='UTC+05:30'), 'date_modified': Timestamp('2024-05-03 14:28:41+0530', tz='UTC+05:30')}),\n",
       "  Document(page_content='Overall, the RAG approach enhances the capabilities of LLMs by enabling them to leverage external knowledge sources in a systematic and efficient manner. This can lead to more powerful and contextually aware applications in various domains, such as natural language understanding, information retrieval, and decision-making.\\nBuilding a Production grade RAG remains a complex and subtle problem. Some of the challenges associated is as follows:-'),\n",
       "  Document(page_content='to bloggers and website owners.'),\n",
       "  Document(page_content='Retrieval-Augmented Generation (RAG)\\xa0is a new approach that leverages Large Language Models (LLMs) to automate knowledge search, synthesis, extraction, and planning from unstructured data sources. This method has gained prominence over the past year due to its ability to enhance LLM applications with contextual information. The RAG data stack consists of several key components:'),\n",
       "  Document(page_content='Content generation can be a time-consuming task for bloggers. Langchain can be leveraged to generate content automatically, either as full articles or as drafts that can be refined by human editors. This feature can significantly speed up the content creation process and allow bloggers to focus on other aspects of their work.\\nPersonalized Recommendations and Dynamic Content'),\n",
       "  Document(page_content='End of Post', metadata={'title': 'RAG using Langchain and Groq with COT', 'short_url': 'https://wp.me/pfK043-7', 'date': Timestamp('2024-05-02 11:51:56+0530', tz='UTC+05:30'), 'date_modified': Timestamp('2024-05-02 11:55:21+0530', tz='UTC+05:30')}),\n",
       "  Document(page_content='Langchain: The Future of Conversational AI in WordPress Blogs'),\n",
       "  Document(page_content='Step 3: Tools')],\n",
       " 'answer': 'I can help you with that! Your post discusses Retrieval-Augmented Generation (RAG), a new approach that leverages Large Language Models (LLMs) to automate knowledge search, synthesis, extraction, and planning from unstructured data sources.'}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wpbotvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
